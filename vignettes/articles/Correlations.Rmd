---
title: "Correlations"
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE}
library(tidyverse)
library(protoClassification)

theme_set(
  theme_light(base_family = "Avenir Next Condensed") +
    theme(strip.background = element_rect(fill = "#4C4C4C"))
)

```

One of the basic points we are pursuing is that the *composition* of categories comes from the wider correlation structure between the various properties we *could* use to categorize someone or something. It also result from the wider prevalence of those same properties.

*Note. I refer to "composition effects" in a separate notebook, these are the changes in the composition of features related to a change in the classification model.*

## Data Simulation

In order to simulate the data we use in this paper, we generate correlated binary observations from two parameters:

1.  The marginal probabilities of $\mathbf x$.
2.  The correlation structure of $\mathbf x$.

```{r}
set.seed(3)
K <- 12

rho <- rlkjcorr(1, K, eta = 1 / 4) ## use this to generate random correlation matrices
marginals <- rbeta(K, 2, 3) # use this to generate random probabilities

corrplot::corrplot.mixed(
  rho,
  tl.col = "black"
)

marginals |>
  enframe() |>
  ggplot(aes(name, value)) +
  geom_col(width = 1 / 3) +
  labs(x = NULL, y = "marginal probability")
```

Now we can simulate data.

```{r}
obs <- 5e3 ## high for precision

set.seed(10)
d <- make_binary_data(marginals, rho, obs)
params <- get_data_params(d) ## I will use this later.
d
```

*Note. The following probably belongs in an appendix, since it is very boring.*

The trick is to take the marginal probabilities for $\mathbf x$ and map them to a "latent Normal space" using a quantile function—i.e., the inverse of the standard normal cumulative distribution function.

$$
\boldsymbol \mu = \Phi^{-1} (\mathbf p)
$$

```{r}
mu <- qnorm(marginals, mean = 0, sd = 1)
mu
```

We then used the correlation matrix $\mathbf R$ and $\mu$ to generate 5,000 draws from a multivariate normal distribution.

$$
\mathbf z \sim \text{MVNormal}\left(\boldsymbol \mu, \mathbf R \right)
$$

```{r}
z <- mvtnorm::rmvnorm(obs, mean = mu, sigma = rho)
dim(z)
```

Finally, we turn these latent values into binary features into probabilities by applying the normal cumulative distribution function (CDF) to them. Then, if the probability is larger than $0.5$ we turn that feature into a one, otherwise we turn it into a zero.

$$
x_k = \begin{cases}
1 &\text{if} &\Phi(z_k) > 0.5 \\
0 &\text{if} &\Phi(z_k) < 0.5
\end{cases}
$$

```{r}
x <- stats::pnorm(z) > 0.5
x[] <- as.integer(x)
head(x, n = 10) # first 10 rows
```

That's it. That is how you generate correlated binary data. This might seem a bit weird, but it is a common data generation process. If you were to calculate the "tetrachoric correlation" of `x` using `psych::tetrachoric(x)` you would find that the result is essentially the same as our original correlation matrix $\mathbf R$.

## Prototype Model

Now we just need to finish up specifying the parameters in the Prototype Model.

1.  The attention weights $\mathbf w$ (one for each feature).
2.  The sensitivity parameters $\boldsymbol \gamma$ (one for each category).
3.  The prototypes $\mathbf p$ (one for each category).

In this example we use an vector of attention weights that basically corresponds to a "rule."

```{r}
w <- rep(0, K)
w[[1]] <- 1

g <- c(10, 10)

prototypes <- list(
  P1 = rep(1, K),
  P2 = rep(0, K)
)

# Compute ----

baseline <- compute(d, prototypes, w, g)
baseline
```

When all attention is placed on a single feature we get to see something very intuitive about the compositions of all the other features. From now on I am going to refer to this single feature as $k^\star$.

For starters, here are a few things.

1.  The proportion of $c_1$ is the same as the proportion of $x_k^\star$.

    $$
    \Pr(C = c_1) = \Pr(X_k = x_k^\star)
    $$

2.  The probability of $x_k^\star$ given $c_1$ is one.

    $$
    \Pr(X_k = x_k^\star \mid C = c_1) = 1
    $$

    This should be intuitive but you can check for yourself:

    ```{r}
    round(conditionalProbs(baseline, "features"), 3)
    ```

3.  The composition of the other features within the $c_1$ category is basically a function the marginal probabilities of $\mathbf x$ and the correlation matrix $\mathbf R$.

    *I will expand on this later.*

For now, this is how we can visualize the composition of all features within each category.

```{r}
probs <- summary(baseline, s = 5e3) # high s for precision

plot_data <- probs$conditional$features |>
  rownames_to_column("C") |>
  pivot_longer(!C, names_to = "x", values_to = "conditionalProb") |>
  left_join(enframe(probs$marginal$features, "x", "marginal")) |>
  left_join(enframe(params$rho[which.max(w), ], name = "x", value = "corr")) |>
  mutate(x = factor(x, levels = paste0("x", 1:K)))

plot_data |>
  ggplot(aes(x, y = marginal)) +
  geom_segment(
    aes(yend = conditionalProb),
    arrow = arrow(length = unit(0.015, "npc"))
  ) +
  facet_wrap(~C) +
  labs(
    x = NULL,
    y = "probability",
    title = "Feature Composition",
    subtitle = "Change in probability after conditioning on C"
  )
```

This plot shows the change in probability from $\Pr(X_k)$ to $\Pr(X_k \mid C)$.

The following plot replaces the horizontal axis with correlations between $x_k$ the "exclusive feature" $x_k^\star$.

```{r}
plot_data |>
  ggplot(aes(corr, marginal)) +
  geom_segment(
    aes(yend = conditionalProb),
    arrow = arrow(length = unit(0.1, "cm"))
  ) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~C) +
  labs(
    title = "Change in Probability",
    subtitle = expression(
      "As a function of the Correlation between" ~ x[k] ~ "and" ~ x[k]^"*"
    ),
    x = expression(rho[kk^"*"]),
    y = "probability"
  )
```

A **discovery** I made doing this simulations is that the *change* in probability of $x_k$ after conditioning on $C$ is roughly a linear function of the correlation between $x_k$ and $x_k^\star$.

$$
\Delta P = \Pr(X_k \mid C) - \Pr(X_k)
$$

```{r}
plot_data |> 
  mutate(change = conditionalProb - marginal) |> 
  ggplot(aes(corr, change)) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_smooth(method = "lm", fill = "skyblue") +
  geom_point() +
  facet_wrap(~C) + 
  labs(
    title = "Change in Probability vs. Correlation",
    y = expression(Delta ~ P), x = expression(rho[kk^"*"])
  )
```

This relationship is even cleaner when we remove $\rho_{k^\star k^\star}$.

```{r}
exclusive_feature <- colnames(baseline$data)[which.max(w)]

plot_data |> 
  mutate(change = conditionalProb - marginal) |> 
  filter(x != exclusive_feature) |> 
  ggplot(aes(corr, change)) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_smooth(method = "lm", fill = "skyblue") +
  geom_point() +
  facet_wrap(~C) + 
  labs(
    title = "Change in Probability vs. Correlation",
    y = expression(Delta ~ P), x = expression(rho[kk^"*"])
  )
```

## The Uncorrelated World

If what we say about the change in probabilities being connected to the correlations between $x_k^\star$ and all other $x_k$, then we would not expect to see anything if we generate uncorrelated data, right?

```{r}
d_uncorr <- make_binary_data(marginals, diag(K), obs)
d_uncorr
uncorrelated <- compute(d_uncorr, prototypes, w, g)
probs_uncorrelated <- summary(uncorrelated, s = 4e3)
```

*Code not shown because it is redundant.*

```{r, echo=FALSE}
params_uncorr <- attr(d_uncorr, "params")

plot_data_uncorr <- probs_uncorrelated$conditional$features |>
  rownames_to_column("C") |>
  pivot_longer(!C, names_to = "x", values_to = "conditionalProb") |>
  left_join(enframe(probs$marginal$features, "x", "marginal")) |>
  left_join(enframe(params$rho[which.max(w), ], name = "x", value = "corr")) |>
  mutate(x = factor(x, levels = paste0("x", 1:K)))

plot_data_uncorr |>
  ggplot(aes(x, y = marginal)) +
  geom_segment(
    aes(yend = conditionalProb),
    arrow = arrow(length = unit(0.015, "npc"))
  ) +
  facet_wrap(~C) +
  labs(
    x = NULL,
    y = "probability",
    title = "Feature Composition",
    subtitle = "Change in probability after conditioning on C"
  )

```

Right!

**In Search for a Simple Function**

So, how do we think about the function linking $p(x \mid c)$ to the parameter values in the simple case in which all attention is given to a single feature $k^\star$ (`baseline`)?

The first thing to notice is that the distance function is very simple.

It goes from this:

$$
\text{dist}(\mathbf{x}_i, \mathbf{p}_A) = \Bigg( \sum_{k = 1}^K w_k | x_{ik} - p_{k} |^r \Bigg)^{\frac{1}{r}}
$$

to this:

$$
\text{dist}(\mathbf{x}_i, \mathbf{p}_A) = \Bigg(  1 \times | x_{ik}^\star - p_{k}^\star |^r \Bigg)^{\frac{1}{r}}
$$

where $\star$ denotes the feature with the single attention and the $r$ parameters are superfluous because the data is binary, and so there are only two possible distances: 0 and 1.

And so this equation:

$$
S (\mathbf{x}_i, \mathbf{p}_A) = \exp \Big(- \gamma \cdot \text{dist} (\mathbf{x}_i, \mathbf{p}_A) \Big)
$$

becomes this

$$
S (\mathbf{x}_i, \mathbf{p}_A) = \begin{cases}
e^ 0 & \text{if}  &x_k^\star=1 \\
e^{-\gamma} & \text{if} &x_k^\star=0 
\end{cases}
$$

And so the probability that $i \in C_A$, when $x_i^\star = 1$ simply becomes this:

$$
\Pr(i \in C_A) = \frac{1}{1 + \underbrace{e^{-\gamma_B}+ e^{-\gamma_C} \dots}_{\text{competing categories}}}
$$

and it is almost close to zero when $x_i^\star = 0$.

If we further assume that $\gamma$ is very high, as is expected when we have more or less exclusionary categories, we have that this probability is approximately either 0 or 1.

But there's more. By the law of total probability we get the following:

$$
\begin{align}
\Pr(C = c_1) &= \overbrace{\Pr(C = c_1 \mid X_k^\star = 1)}^{1} \Pr(X_k^\star = 1) + \overbrace{\Pr(C = c_1 \mid X_k^\star = 0)}^{0} \Pr(X_k^\star = 0) \\
&= \Pr(X_k^\star = 1)
\end{align} 
$$

But this is not what we want, we want $\Pr(X_j = 1 \mid C = c)$.

Thus, we can expand our thing once more using the law of total probability.

$$
\Pr(X_j = 1 \mid C) = \Pr(X_j =1 \mid C, X_k^\star = 1) \Pr(X_k^\star = 1 \mid C) + \Pr(X_j = 1 \mid C, X_k^\star = 0) \Pr(X_k^\star = 0 \mid C)
$$

Given what we know about all of this, we have that $X_j$ and $C$ are *conditionally independent.*

$$
X_j \perp C \mid X_k^\star
$$

This means that $\Pr(X_j \mid C, X_k^\star) = \Pr(X_j \mid X_k^\star)$.

So we can rewrite our conditional probability (using the law of total probability) as:

$$
\Pr(X_j = 1 \mid C) = \Pr(X_j = 1 \mid X_k^\star = 1) \Pr(X_k^\star = 1 \mid C = c_1) + \Pr(X_j = 1 \mid X_k^\star = 0) \Pr(X_k^\star = 0 \mid C = c_1)
$$

Since we established that with high $\gamma$ values: - $\Pr(X_k^\star = 1 \mid C = c_1) \approx 1$ and $\Pr(X_k^\star = 0 \mid C = c_1) \approx 0$ - $\Pr(X_k^\star = 1 \mid C = c_2) \approx 0$ and $\Pr(X_k^\star = 0 \mid C = c_2) \approx 1$

This simplifies to:

$$
\Pr(X_j = 1 \mid C = c_1) \approx \Pr(X_j = 1 \mid X_k^\star = 1) 
$$

Let's verify this.

```{r, eval=FALSE}
exclusive_feature <- colnames(sim_data)[which.max(w2)]
exclusive_feature

sim_data |>
  summarize(across(everything(), mean), .by = all_of(exclusive_feature))

probs_baseline$conditional$features
```

Correct!

**The Connection to Correlations**

The relationship between $\Pr(X_j = 1 \mid X_k^\star)$ and correlations becomes clear when we consider the bivariate normal copula underlying our binary data generation process.

For binary variables generated from a bivariate normal with correlation $\rho_{jk^\star}$, we have:

$$
\Pr(X_j = 1 \mid X_k^\star = 1) = \Phi_2\left(\Phi^{-1}(p_j), \Phi^{-1}(p_{k}^\star); \rho_{jk}^\star \right) / p_{k}^\star
$$

IMPROVE, WITH JOINT PROBABILITY AND BIVARIATE CDF

where $p_j$ and $p_{k}^\star$ are the marginal probabilities, and $\Phi_2$ is the bivariate standard normal CDF.

```{r, eval=FALSE}
bivariateCondProb(params, kstar = 3) |> round(3)
probs_baseline$conditional$features |> round(3)
```

**Visualization of the Relationship**

```{r, eval=FALSE}

# Show correlation coefficient
cor_coef <- cor(comparison_data$correlation, comparison_data$change)
cat("Correlation between ρ and probability change:", round(cor_coef, 3), "\n")
```

**Summary**

This analysis demonstrates that when all attention is focused on a single feature in prototype-based classification:

1.  **Conditional Independence**: Other features become conditionally independent of the category given the exclusive feature: $X_j \perp C \mid X_k^\star$

2.  **Simplified Conditional Probabilities**:

    -   $\Pr(X_j = 1 \mid C = c_1) \approx \Pr(X_j = 1 \mid X_k^\star = 1)$
    -   $\Pr(X_j = 1 \mid C = c_2) \approx \Pr(X_j = 1 \mid X_k^\star = 0)$

3.  **Linear Relationship with Correlations**: The change in probability $\Pr(X_j \mid C) - \Pr(X_j)$ is approximately linear in the correlation $\rho_{jk^\star}$ between feature $j$ and the exclusive feature $k^\star$.

4.  **Theoretical Foundation**: This relationship stems from the bivariate normal copula underlying the binary data generation process, making the prototype model's behavior predictable and interpretable.

This provides both theoretical insight and practical understanding of how prototype-based classification behaves under extreme attention weighting scenarios.

## Extra

Modifying the correlation coefficient...

`transform_rho`

If $A \perp B$, then $P(A \mid B) = P(A)$, meaning that the difference between both quantities is zero. Knowing which category a person belongs to increaess or decreases the probability of the feature.

DO A MULTI MULTI SIMULATION

Remember to keep separate the idea of compositional effects and changing compositions.

Compositional effects come from changes in model parameters.

Changing compositions come from changes in data parameters (marginals and correlations)
