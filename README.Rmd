---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# protoClassification

<!-- badges: start -->

[![R-CMD-check](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml)

<!-- badges: end -->

Install the development version of protoClassification from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("acastroaraujo/protoClassification")
```

## Get Started

To simulate a dataset you need to create to decide a couple of things first.

1.  The number of $K$ dimensions.
2.  The marginal probabilities for each dimension.
3.  A correlation matrix for the dimensions.

```{r}
library(protoClassification)
set.seed(1)
K <- 6 # 1st step
marginals <- rbeta(K, 2, 2) # 2nd step
rho <- rlkjcorr(1, K, eta = 1) # 3rd step

```

**Generate data.**

```{r}
set.seed(1)
sim_data <- make_binary_data(marginals, rho, obs = 1e3)
sim_data
```

*Note. The parameters are stored as the `params` attribute in the output.*

We can verify that the column means *roughly* correspond to the marginal probabilities.

```{r}
colMeans(sim_data)
```

In order to verify that the data follows the correlation structure in `rho` you would have to calculate a "[tetrachoric correlation](https://en.wikipedia.org/wiki/Polychoric_correlation)."

```{r, warning=FALSE}
psych::tetrachoric(sim_data)$rho
```

Additional stuff for Prototype Classification Model:

-   `w` a vector of attention weights for each k

-   `P` a list of prototypes, one per category.

-   `g` (gamma) sensitivity parameter.

```{r}
set.seed(1)
w <- runif(K)
w <- w / sum(w)
g <- 10
```

Calculate distance and similarity for one prototype at a time:

```{r}
d <- calculateDistSim(
  data = sim_data,
  P = rep(1, K),
  w = w,
  g = g
)

str(d)
```

Calculate distance, similarity, and probabilities for multiple prototypes at the same time:

```{r}
prototypes <- list(
  P1 = rep(1, K),
  P2 = rep(0, K),
  P3 = rep(1:0, K / 2)
)

g <- rep(10, 3)

out <- compute(sim_data, prototypes, w, g)
out
```

`consolidate()` the previous output into a single data frame for easier visualization.

```{r}
d <- consolidate(out)
str(d)
```

*Note. Since only binary data is implemented, there is no difference between Manhattan and Euclidean distance!*

## Marginal and Conditional Probabilities

So far, a single simulation requires the marginal probabilities for each element of $\mathbf{x}$ to be specified at the outset.

```{r}
colMeans(out$data) # cf. `marginals` argument in `make_binary_data()`
```

The more relevant piece of information we get from the `compute()` function is the `.$probabilities` object, which calculates the probability that any given individual in our simulated dataset will belong to each of the prototype categories.

This allows us to calculate the marginal probabilities for each category.

```{r}
colMeans(out$probabilities)
```

With some ingenuity, we can use this information to get *conditional probabilities* too.

$$
\Pr(X_k = 1 \mid C = c)
$$

```{r}
conditionalProbs(out, "features")
```

$$
\Pr(X_k = 0 \mid C = c)
$$

```{r}
1 - conditionalProbs(out, "features")
```

$$
\Pr(C = c \mid X_k)
$$

```{r}
conditionalProbs(out, type = "categories")
```

Alternatively, it's easier to use the `summary()` function to extract all conditional and marginal probabilities.

```{r}
probs <- summary(out)
probs
```
