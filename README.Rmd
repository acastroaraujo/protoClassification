---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# protoClassification

<!-- badges: start -->

[![R-CMD-check](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml)

<!-- badges: end -->

Install the development version of protoClassification from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("acastroaraujo/protoClassification")
```

## Get Started

To simulate a dataset you need to create to decide a couple of things first.

1.  The number of $K$ dimensions.
2.  The marginal probabilities for each dimension.
3.  A correlation matrix for the dimensions.

```{r}
library(protoClassification)
set.seed(1)
K <- 6 # 1st step
marginals <- rbeta(K, 2, 2) # 2nd step
rho <- rlkjcorr(1, K, eta = 1) # 3rd step

```

**Generate data.**

```{r}
set.seed(1)
sim_data <- make_binary_data(marginals, rho, obs = 1e3)
sim_data
```

*Note. The parameters are stored as the `params` attribute in the output.*

We can verify that the column means *roughly* correspond to the marginal probabilities.

```{r}
colMeans(sim_data)
```

In order to verify that the data follows the correlation structure in `rho` you would have to calculate a "[tetrachoric correlation](https://en.wikipedia.org/wiki/Polychoric_correlation)."

```{r, warning=FALSE}
psych::tetrachoric(sim_data)$rho
```

Additional stuff for Prototype Classification Model:

-   `w` a vector of attention weights for each k

-   `P` a list of prototypes, one per category.

-   `g` (gamma) sensitivity parameter.

```{r}
set.seed(1)
w <- runif(K)
w <- w / sum(w)
g <- 10
```

Calculate distance and similarity for one prototype at a time:

```{r}
d <- calculateDistSim(
  P = rep(1, K), 
  w = w, 
  data = sim_data, 
  g = g
)

str(d)
```

Calculate distance, similarity, and probabilities for multiple prototypes at the same time:

```{r}
prototypes <- list(
  P1 = rep(1, K),
  P2 = rep(0, K),
  P3 = rep(1:0, K / 2)
)

g <- rep(10, 3)

out <- compute(sim_data, prototypes, w, g)
out
```

`consolidate()` the previous output into a single data frame for easier visualization.

```{r}
d <- consolidate(out)
str(d)
```

*Note. Since only binary data is implemented, there is no difference between Manhattan and Euclidean distance!*

## Marginal and Conditional Probabilities

So far, a single simulation requires the marginal probabilities for each element of $\mathbf{x}$ to be specified at the outset.

```{r}
colMeans(out$data) # cf. `marginals` argument in `make_binary_data()`
```

The more relevant piece of information we get from the `compute()` function is the `.$probabilities` object, which calculates the probability that any given individual in our simulated dataset will belong to each of the prototype categories.

This allows us to calculate the marginal probabilities for each category.

```{r}
colMeans(out$probabilities)
```

With some ingenuity, we can use this information to get *conditional probabilities* too.

$$
\Pr(X_k = 1 \mid C = c)
$$

```{r}
conditionalProbs(out, "features")
```

$$
\Pr(X_k = 0 \mid C = c)
$$

```{r}
1 - conditionalProbs(out, "features")
```

$$
\Pr(C = c \mid X_k)
$$

```{r}
conditionalProbs(out, type = "categories")
```

Alternatively, it's easier to use the `summary()` function to extract all conditional and marginal probabilities.

```{r}
probs <- summary(out)
probs
```

## Compositional Effects

Our main goal is to compare different probabilities across different parameters values. In particular, we can measure the "compositional effects" that result from different parameter values by keeping track of these conditional probabilities: $\Pr(\mathbf{x} \mid C, \Omega)$, where $\Omega$ is a short-hand way of referring to all parameters in the prototype model.

Here I provide a few examples.

```{r}
set.seed(9)
# Basic Workflow ----

K <- 10 
obs <- 1e3

# Data ----

mu <- rbeta(K, 2, 2) 
rho <- rlkjcorr(1, K, eta = 1)

sim_data <- make_binary_data(mu, rho, obs)

# Prototype Parameters ----

w <- runif(K)
w <- w / sum(w)

g <- c(10, 10)

prototypes <- list(
  P1 = rep(1, K),
  P2 = rep(0, K)
)

# Compute ----

out <- compute(sim_data, prototypes, w, g)
probs <- summary(out, s = 1e3)
```

**Changing** $\gamma$

```{r}
# This should make the second category more inclusive
new_out <- compute(sim_data, prototypes, w, g = c(10, 5))
new_probs <- summary(new_out, s = 1e3)
```

This should increase the marginal probabilities of the second category.

```{r}
probs$marginal$categories
new_probs$marginal$categories
```

And these are the compositional effects:

```{r}
probs$conditional$features
new_probs$conditional$features

# diff-in-prob
new_probs$conditional$features - probs$conditional$features
# risk ratio
new_probs$conditional$features / probs$conditional$features
```

**Changing the "attention weights"**

1st, we can make the attention weights more selective, meaning that classification becomes more rule-like.

```{r}
w2 <- temperature(w, temp = 1/4) # cf. "temperature sampling"
barplot(w, names.arg = seq_along(w), main = "Original Weights")
barplot(w2, names.arg = seq_along(w), main = "New Weights")

new_out <- compute(sim_data, prototypes, w = w2, g)
new_probs <- summary(new_out, s = 1e3)
```

The change in compositional effects should make it such that the new $p(x_k \mid c)$ are very high when $x_k$ is highly weighted.

```{r}
probs$conditional$features
new_probs$conditional$features

# diff-in-prob
new_probs$conditional$features - probs$conditional$features
# risk ratio
new_probs$conditional$features / probs$conditional$features
```

At the extreme, when attention is placed exclusively on one feature.

We should expect $p(x_k \mid c)$ to be 1.

```{r}
w2 <- vector("double", length(w)) # all attention on dimension 3
w2[[3]] <- 1
barplot(w2, names.arg = seq_along(w2))

new_out <- compute(sim_data, prototypes, w = w2, g)
new_probs <- summary(new_out, s = 1e3)

new_probs$conditional$features
```

Furthermore, marginal probability of $c_1$ should converge to the marginal probability of that feature with exclusive attention.

```{r}
new_probs$marginal
```

Compositional Effects:

```{r}
# diff-in-prob
new_probs$conditional$features - probs$conditional$features

# risk ratio
new_probs$conditional$features / probs$conditional$features
```

## Back to Correlations

When all attention is placed on one feature, like in the previous example, we get to see something very intuitive about the compositions of all the other features.

If we compare

```{r}
xCondC1 <- new_probs$conditional$features["C1", ]
x <- new_probs$marginal$features

xCondC1 - x ## difference from baseline marginal probability
```

Look at the original correlation matrix:

```{r}
rho <- attr(new_out$data, "params")$rho
round(rho["x3", ], 3)
```

It seems like this difference from baseline is a linear function of the correlations between the features.

```{r}
d <- data.frame(corr = rho["x3", ], diff = xCondC1 - x)
plot(d)
```

This is certainly not the case when the attention weights are more spread out.

```{r}
xCondC1 <- probs$conditional$features["C1", ]
x <- probs$marginal$features
rho <- attr(out$data, "params")$rho

d <- data.frame(corr = rho["x3", ], diff = xCondC1 - x)
plot(d)
```

The mission is to find a formula that explains $p(x\mid c)$ as a function of the simulation parameters!
