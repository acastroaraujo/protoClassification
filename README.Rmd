---
output: github_document
editor_options: 
  chunk_output_type: console
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# protoClassification

<!-- badges: start -->
[![R-CMD-check](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/acastroaraujo/protoClassification/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

Install the development version of protoClassification from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("acastroaraujo/protoClassification")
```

```{r overview, echo=FALSE}
knitr::include_graphics("overview.png")
```

### Get Started

To simulate a dataset like the one in the Figure you need to create to decide a couple of things first.

1. The number of $K$ dimensions.
2. The marginal probabilities for each dimension.
3. A correlation matrix for the dimensions.

```{r}
library(protoClassification)
set.seed(1)
K <- 6 # 1st step
marginals <- rbeta(K, 2, 2) # 2nd step
rho <- rlkjcorr(1, K, eta = 1) # 3rd step

nms <- paste0("k", 1:K)
names(marginals) <- nms
dimnames(rho) <- list(nms, nms)

round(rho, 2)
round(marginals, 2)
```

**Generate data.**

```{r}
set.seed(1)
X <- make_binary_data(marginals, rho, obs = 1e3)
head(X, n = 10)
```

Verify that the column means _roughly_ correspond to the marginal probabilities.

```{r}
colMeans(X) |> round(2)
```

In order to verify that the data follows the correlation structure in `rho` you would have to calculate a "[tetrachoric correlation](https://en.wikipedia.org/wiki/Polychoric_correlation)."

```{r, warning=FALSE}
psych::tetrachoric(X)$rho |> round(2)
```

Additional stuff for Prototype Classification Model:

- `g` (gamma) sensitivity parameter
- `w` a vector of attention weights for each k
- `P` a list of prototypes

```{r}
set.seed(1)
w <- runif(K)
w <- w / sum(w)
g <- 10
```

Calculate distance and similarity for one prototype at a time:

```{r}
d <- calculateDistSim(
  P = rep(1, K), 
  w = w, 
  data = X, 
  g = g
)

str(d)
```

Calculate distance, similarity, and probabilities for multiple prototypes at the same time:

```{r}
prototypes <- list(
  P1 = rep(1, K),
  P2 = rep(0, K),
  P3 = rep(1:0, K / 2)
)

out <- compute(prototypes, w, X, g = 10, r = 1)
out
```

`consolidate()` the previous output into a single data frame for easier visualization.

```{r}
d <- consolidate(out)
str(d)

library(ggplot2)

d |> 
  ggplot(aes(dist1, sim1)) + 
  geom_jitter(height = 0, width = 1/100, alpha = 1/4) + 
  theme_light()
```


The more relevant piece of information coming from the `compute()` function is the `.$probabilities` object.

```{r}
out$probabilities |> 
  head(n = 10)
```

With this you can classify each row in the simulated dataset and then get conditional probabilities for each $K$ feature.

_Deterministically:_

```{r}
category <- apply(out$probabilities, 1, which.max)
lapply(split(out$data, category), colMeans)
```

_Probabilistically:_

```{r}
category <- apply(out$probabilities, 1, \(x) {
  sample(seq_along(x), size = 1, prob = x)
})
lapply(split(out$data, category), colMeans)
```

Or using the `conditionalProbs()` function.

```{r}
conditionalProbs(out, .sample = TRUE)
```

The point is to compare different probabilities across different parameters values (i.e., compositional effects).

For example:

```{r}
w_unif <- temperature(w, 5) # make weights more uniform
w_unif

compute(prototypes, w_unif, X, g = 10, r = 1) |> 
  conditionalProbs() 


w2 <- vector("double", length(w)) # all attention on dimension 2
w2[[2]] <- 1
w2

compute(prototypes, w2, X, g = 10, r = 2) |> 
  conditionalProbs() 
```

To do:

- Allow `g` to vary by category. 
- Figure out when "r = 1" or "r = 2" matters.
- Figure out a best way to measure compositional effects (e.g., relative risk ratio, difference in probabilities)
